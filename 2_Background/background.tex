% BACKGROUND

\subsection{Reinforcement Learning}
Reinforcement learning (RL) deals with goal-directed decision-making in an unknown environment, modeled as a Markov decision process (MDP). An MDP, denoted as $\mathcal{M} = (\mathcal{S}, \mathcal{A}, T, r, \rho_{0}, \gamma)$, consists of finite sets for environment states $\mathcal{S}$ and available actions $\mathcal{A}$, probabilistic state transition functions $T : \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}^{+}$, immediate reward functions $r : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$, and the initial state distribution $\rho_0$. At each time step $t$, the agent observes the current state $s_t \in \mathcal{S}$, and selects an action $a_t \in \mathcal{A}$ based on its policy. Then, the environment returns a scalar reward $r_t:=r(s_t, a_t)$ and transitions to the next state $s_{t+1}$ using the dynamics distribution $T(s_{t+1} | s_t, a_t)$. The agent's goal is to learn an optimal policy $\pi: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}^{+}$ that maximizes the expected discounted cumulative return: 

\begin{equation}
    J(\pi) := \mathbb{E}_{\pi, T, \rho_0} \left[ \sum_{t=0}^{\infty} \gamma^t r_t \right]
\end{equation}
 
Here, $\rho_0$ represents the initial state distribution, characterizing the probability distribution over initial states, and $\gamma$ is the discount factor ensuring that the objective converges to a finite sum as $t$ grows. The reward serves as a guiding signal, indicating whether the agent's actions align with reaching the goal state, and the policy is the decision-making component.




\subsection{ Offline Reinforcement Learning}

In offline reinforcement learning, the environment is unavailable for interaction, and instead, the agent must rely on a fixed dataset $\mathcal{D}$. This dataset $\mathcal{D}$ is denoted as a set of tuples $(s, a, r, s')$, trajectories collected by a behavioral policy $\pi_{\beta}$. In most settings, the behavioral policy is unknown. The actions are derived from the behavior policy $a_t \sim \pi_{\beta}( \cdot | s_t)$, the states are sampled from a distribution induced by the behavior policy $s_t \sim \rho_{\pi_{\beta}}(\cdot)$, the next state is determined by the transition dynamics $s_{t+1} \sim T(\cdot | s_t, a_t)$, and the reward is a function of state and action $r_t = \mathcal{R}(s_t, a_t)$. The objective in offline RL remains the same as online RL: learning a performant policy $\pi$ given the static dataset $\mathcal{D}$.

 
\subsection{Approaches}

Several key divides exist within RL approaches. RL can be distinguished as on-policy or off-policy methods. In on-policy RL (equivalent to online RL), the agent interacts with the environment and learns a policy- a mapping from states to actions. After each interaction, the collected data gets discarded, and new data is retrieved to improve the agent’s learning. Off-policy reinforcement learning allows the agent to gather information by interacting with the environment and storing the data in a buffer. This buffer is used to update the policy in iterations. On-policy methods generate data for learning the current target policy, while off-policy approaches allow learning from data generated by a potentially different behavior policy, enabling better reuse of experience data. Another subdivision in the aforementioned categories is determined by the presence of a “model.” A model refers to reversible access to an environment’s Markov Decision Process (MDP) dynamics. Model-free reinforcement learning approaches have irreversible access to such dynamics, meaning they do not explicitly build a model of the environment and rely solely on interaction experiences to make decisions and learn optimal policies. One popular model-free method is Q-learning- a technique that aims to find an optimal action-selection policy for an agent operating within an environment. It achieves this by iteratively estimating the value of state-action pairs, known as Q-values, and updating them based on observed rewards and transition dynamics, ultimately guiding the agent toward selecting actions that maximize cumulative expected rewards over time.

In contrast, model-based reinforcement learning approaches have reversible access to the MDP dynamics, which allows the agent to repeatedly plan forward from the same state. In addition to learning by interacting, the agent builds models of the transition dynamics and reward functions as proxies for the real environment. These constructed models enable the agent to plan accordingly before acting, simulate the consequences of different actions, and generally allow for more exploration of out-of-distribution (OOD) regions. Empirical findings indicate that model-based offline RL approaches outperform the majority of existing model-free offline RL methods in various ways. \\


% Model-based reinforcement learning (MB-RL) is a prominent approach within the broader field of reinforcement learning. At its core, MB-RL deals with autonomous decision-making and learning in environments modeled as Markov decision processes (MDPs). An MDP, formally denoted as \((\mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R}, \gamma)\), encompasses a finite set of states \(\mathcal{S}\), a limited set of actions \(\mathcal{A}\) available to the agent, transition dynamics \(\mathcal{T}\) representing state transition probabilities, an instantaneous reward function \(\mathcal{R}\), and a discount factor \(\gamma\) that accounts for the importance of future rewards. In MB-RL, the agent aims to learn an optimal policy \(\pi\) that maximizes the expected cumulative reward over time, commonly expressed as:

% \[
% \sum_{t=0}^{\infty} \gamma^t \mathcal{R}(s_t, a_t)
% \]

% However, what distinguishes MB-RL from its counterpart, model-free RL, is its utilization of an internal model of the environment, often referred to as the "dynamics model." This model serves as a representation of how the environment behaves, allowing the agent to simulate and plan its actions more effectively. By using this dynamics model, MB-RL algorithms can generate hypothetical trajectories, assess potential action sequences, and make informed decisions that maximize expected rewards. This approach is particularly advantageous when interactions with the real environment are costly or dangerous, such as in robotics or autonomous control scenarios.

% The overarching goal of MB-RL is to strike a balance between exploration and exploitation by leveraging the dynamics model. The agent explores and learns about the environment through interactions but also relies on its internal model for efficient decision-making. MB-RL methods come in various forms, including model predictive control, value iteration, and Monte Carlo Tree Search. These techniques differ in how they handle the dynamics model and plan actions.

% MB-RL has found applications in a wide range of domains, including robotics, autonomous vehicles, game-playing, and recommendation systems. Its capacity to incorporate prior knowledge and simulated experiences enables it to address complex real-world challenges where gathering data through trial and error may be impractical. As MB-RL continues to evolve, it holds the promise of developing intelligent systems that can adapt and excel in dynamic and uncertain environments.