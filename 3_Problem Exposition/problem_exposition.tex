\section{Distributional Shift}

 The distribution shift is an umbrella term encompassing the idea of generalizability. Various texts use both terms interchangeably, which can lead to ambiguity. (in assessing generalization in deep RL, page 3 Leike et al. 2017). In offline RL, the agent learns from a fixed dataset collected from a behavior policy, and the goal is to improve upon this behavior policy. However, the dataset may not entirely represent the distribution of states and actions that the agent will encounter during deployment, leading to a considerable mismatch between the learned policy’s estimation of state-action values and its real-world performance. (Xu et al., 2022). We provide a concise definition for this term below.
\begin{flushleft}
    \textit{Distribution shift is the discrepancy between the data distribution used during training and the distribution of states and actions encountered during the deployment of the learned policy.}
\end{flushleft}


The inconsistency between the collected data and the real environment can cause training instabilities in offline RL methods. Improper state-action space exploration may induce the agent to learn poor behaviors in sparse regions, i.e., regions in the data lacking informative states. For example, they can overestimate the value of their actions (Kumar, 2020). When training on complex and multi-modal data distributions, overestimated values can lead to suboptimal performance. In Q-learning, the agent prioritizes maximizing its Q-value, incentivizing it to take actions that result in high immediate rewards. However, these actions may prevent the agent from reaching the goal state or receiving future rewards. (Kumar, 2020). The learned policy may deviate far beyond the behavior policy by exploiting “fake” regions with high Q-values, ultimately leading to lower expected returns. 

Several approaches aim to address the distributional shift problem in offline RL. Some offline model-free methods to mitigate deviation from the behavioral policy include policy constraints, implicit or conservative Q-learning (CQL), and behavioral regularization. For example, CQL mitigates the overestimation of values induced by the distributional shift (Kumar, 2020) by pushing up Q-values of state-action pairs sampled from D and penalizing those that are not. The agent balances exploring and exploiting by promoting actions the agent is more confident about and penalizing uncertain actions that may induce out-of-distribution behaviors. 

Offline model-based methods primarily tackle divergences between the dataset and the learned policy by estimating model uncertainty, adversarial model training, latent space projections, and employing variants of lenient Q-learning that are less risk-averse compared to their model-free counterparts. These model-based strategies aim to enhance the agent’s adaptability and robustness by improving trajectory estimations generated by their dynamics model, thereby addressing the distributional shift challenge more effectively.

  

\section{Generalization}

 The study of generalization in Deep RL is a well-explored area that continues to evolve. Although many offline RL approaches primarily center around addressing the shift between training and test distributions, recent developments in this field have underscored a growing emphasis on improving the generalization mechanisms of these algorithms to create algorithms that are more robust to distributional variations. Nevertheless, Kirk et al. argue that improving “generalization” can be ambiguous if the assumptions or the type of generalization are not clearly defined. This ambiguity makes it difficult or unlikely to determine whether proposed methods achieve generalization and to what degree. Therefore, formalization of generalization in the context of distributional shift, overfitting, and other relevant adaptation properties is crucial. 

RL algorithms are primarily designed and assessed on their ability to maximize performance within predefined conditions and environments (training settings) rather than their ability to learn representations and cultivate a deeper comprehension of their experiences and surroundings for generalizing to novel and unanticipated scenarios (test settings). Reality is stochastic and non-stationary, and behaviors captured in the offline dataset often cannot represent or capture these variations. Therefore, overfitting to specific training environments can be detrimental to performance, even causing poor performance in similarly constructed test distributions. Kumar et al. (workflow paper) explicate overfitting in conservative offline RL methods such as CQL and adapt it for behavioral regularization methods. They propose a definition of overfitting in offline RL that is analogous to the definition in supervised learning- overfitting corresponds to a low value for the empirical training loss but poor actual policy performance $J(\pi)$, where the empirical training loss is given by the <q learning equation 1> and $J(\pi)$ denotes the average return of the policy in the MDP. The authors postulate that when the estimated Q-values show a non-monotonic trend, it suggests that the learned policy begins to match the behavioral policy used for data collection closely. Due to the behavioral policy’s limited coverage of the state-action space, learning a new policy akin to it results in suboptimal performance in the actual environment.

Given the discussions on distributional shift and overfitting, we can formalize generalizability in offline RL. We adopt a similar line of reasoning to Kirk et al. and define generalizability as follows:

\begin{flushleft}
    \textit{Generalizability is a class of problems within RL that refers to an agent’s performance and adaptability in modified or unseen test distributions. }
\end{flushleft}

In this context, we build upon Kirk et al.’s conceptualization of generalization environments, where they distinguish between IID (In-Distribution) and OOD (Out-of-Distribution) generalization scenarios. In their framework, IID generalization represents environments where the training and testing distributions are identical, while OOD generalization corresponds to situations where these distributions diverge. Kirk et al.’s approach is well-suited for the online RL setting, where agents can access continuous training data and interact with their environment in real time. However, offline learning operates from a dataset of pre-collected experiences from a particular environment, and the same environment is used to assess the quality of the learned policy. In this singleton environment case, we may be more interested in in-domain transfer, where the test environment is a modified version of the training environment. To consolidate this idea, we use the following notation:

\begin{enumerate}
    \item Singleton task, where we are interested in maximizing our performance in a test distribution that is identical to the training distribution. Using \(\mathcal{D}\), we learn \(\hat{M}\) to estimate the true MDP. Here, \(\hat{M}\) and the MDP have the same dynamics.
    \item In a modified singleton task, our focus is on evaluating the performance and adaptability of our agent in a test distribution that involves modified dynamics or reward functions. Instead of measuring performance based on the true MDP, we are more interested in how well the agent performs in an unseen MDP \({M}^*\), where \({M}^*\) is defined as \((\mathcal{S}, \mathcal{A}, \mathcal{T}^*, \mathcal{R}^*, \rho_0, \gamma)\) (Augmented world models paper).
\end{enumerate}



   

\subsection{Generalizability as a measure}


Several studies have contributed to defining and quantifying generalization. For instance, Zhang et al. (in their study on overfitting) use a score that measures the difference between an agent's performance in testing and training scenarios. This approach emphasizes the ability of the agent to generalize beyond its training data. Risk-sensitive measures can also evaluate the agent’s performance under different risk profiles or safety constraints, ensuring that it generalizes well regarding risk tolerance (Garcia \& Fernández, 2015). Packer et al. offer a notable framework for studying generalization in deep RL. They distinguish between three sets of training and test distributions: deterministic (D), random (R), and extreme (E). The deterministic environment represents a fixed and unmodified setting, while the random and extreme environments introduce increasing variations in the sampling of parameters while resetting the environment to its initial state. This setup mirrors real-world tasks, where the random environment captures the distribution of environments from which it is practical and reasonable to gather training data. On the other hand, the extreme environment represents edge cases, which are uncommon environments that are not encountered during training but need to be handled during deployment. Packer et al. define generalization through two measures: interpolation and extrapolation performance. Interpolation assesses performance when training and evaluating an agent on random environments (R), while extrapolation performance is determined by taking the geometric mean of the performance when training on D and evaluating on R and E, as well as training on R and evaluating on E. Interpolation evaluates how well agents can perform in test scenarios that are similar to the ones encountered during training (Point 1 from the previous section), while extrapolation tests the ability of agents to adapt to unfamiliar environments with significantly different parameters compared to their training parameters.




- Generalizability as a measure

- Constrain (most model-free methods use this) vs adaptation

- Interpolation vs extrapolation

- Cross-domain vs. in-domain vs zero-shot

- Dynamics adaptation vs data augmentation